{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nDocumentation notes:\\n\\n- By a laws' subject, I mean P1269 (facets of).\\n\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Documentation notes:\n",
    "\n",
    "- By a laws' subject, I mean P1269 (facets of).\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "import requests\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespace\n",
    "ns = {\n",
    "    'srw_dc': 'info:srw/schema/1/dc-schema',\n",
    "    'dc'    : 'http://purl.org/dc/elements/1.1/',\n",
    "    'srw'   : 'http://www.loc.gov/zing/srw/',\n",
    "    'xsi'   : 'http://www.w3.org/2001/XMLSchema'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Law class that will store all of the scraped data.\n",
    "\"\"\"\n",
    "class law:\n",
    "    def __init__(self):  \n",
    "        self.tipoDocumento = \"\"\n",
    "        self.date = \"\"\n",
    "        self.urn = \"\"\n",
    "        self.localidade = \"\"\n",
    "        self.autoridade = \"\"\n",
    "        self.title = \"\"\n",
    "        self.description = \"\"\n",
    "        self.identifier = \"\"\n",
    "        self.subject = []\n",
    "        \n",
    "    def print_self(self):\n",
    "        return (self.tipoDocumento + \"*\" + self.date + \"*\" + self.urn + \\\n",
    "               \"*\" + self.localidade + \"*\" + self.autoridade + \"*\" + self.title + \\\n",
    "               \"*\" + self.description + \"*\" + self.identifier + \"*\" + \"*\".join(self.subject) + \"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "Lexicon class that will store all subjects covered by laws, as well\n",
    "as how many laws include it.\n",
    "\"\"\"\n",
    "class lexicon:\n",
    "    def __init__(self, word, count):\n",
    "        self.word = word\n",
    "        self.count = count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts the relevant informations from the current item. Creates a law \n",
    "object and saves said data on it. \n",
    "\"\"\"\n",
    "def get_values(x):\n",
    "    new_law = law()\n",
    "    for i in x.iter():\n",
    "        tag = i.tag\n",
    "        if tag == tD:\n",
    "            new_law.tipo = i.text\n",
    "        elif tag == date:\n",
    "            new_law.date = i.text\n",
    "        elif tag == urn:\n",
    "            new_law.urn = i.text\n",
    "        elif tag == localidade:\n",
    "            new_law.localidade = i.text\n",
    "        elif tag == autoridade:\n",
    "            new_law.autoridade = i.text\n",
    "        elif tag == title:\n",
    "            new_law.title = i.text\n",
    "        elif tag == description:\n",
    "            if (i.text is not None):\n",
    "                new_law.description = i.text\n",
    "        elif tag == ID:\n",
    "            new_law.identifier = i.text\n",
    "        elif tag == subject:\n",
    "            subjects = [x.strip() for x in re.split('\\s[,.]\\s', i.text)]\n",
    "            unique_subjects = list(set(subjects))\n",
    "            for i in range(0, len(unique_subjects)):\n",
    "                if (\" .\" in unique_subjects[i]):\n",
    "                    unique_subjects[i] = unique_subjects[i][:-2]\n",
    "            new_law.subject = unique_subjects\n",
    "    return new_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_test(x):\n",
    "    new_law = law()\n",
    "    for i in x.iter():\n",
    "        tag = i.tag\n",
    "        if (\"}\") in tag:\n",
    "            tag = tag.split(\"}\")[1]\n",
    "        if tag in attributes:\n",
    "            if (tag != \"subject\"):\n",
    "                setattr(new_law, tag, i.text)\n",
    "            else:\n",
    "                subjects = [x.strip() for x in re.split('\\s[,.]\\s', i.text)]\n",
    "                unique_subjects = list(set(subjects))\n",
    "                for i in range(0, len(unique_subjects)):\n",
    "                    if (\" .\" in unique_subjects[i]):\n",
    "                        unique_subjects[i] = unique_subjects[i][:-2]\n",
    "                setattr(new_law, tag, unique_subjects)\n",
    "    return new_law\n",
    "\n",
    "attributes = (\"tipoDocumento\", \"date\", \"urn\", \"localidade\", \"autoridade\", \"title\", \"description\", \"identifier\", \"subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gets the lexicon of subjects of all laws. Some laws list the \n",
    "same exact topic under it's subject twice. Those repetitions are discarded.\n",
    "\"\"\"\n",
    "def get_lexicon(laws):\n",
    "    lex = []\n",
    "    for l in laws:\n",
    "        for s in l.subject:\n",
    "            lex.append(s)\n",
    "    lexicon = collections.Counter(lex)\n",
    "    lexicon = sorted(lexicon.items(), key = lambda lex: lex[1], reverse = True)\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prints the lexicon into file_name_lexicon.txt, in the format:\n",
    "total_of_occurrences*word\n",
    "\"\"\"\n",
    "def print_lexicon(lexicon, file_name):\n",
    "    file_lex = open(file_name + \"_lexicon.txt\", \"w\")\n",
    "    for key, value in lexicon:\n",
    "        file_lex.write(str(value) + \"*\" + key + \"\\n\")\n",
    "    file_lex.close()\n",
    "    \n",
    "    \n",
    "\"\"\"    \n",
    "Prints all data scraped from the laws into file_name.txt, \n",
    "separeted by *, in the format:\n",
    "Type_of_document*date*urn*locality*authority*title*description*identifier*subjects\n",
    "\"\"\"\n",
    "def print_scraped_info(laws, file_name):\n",
    "    file = open(file_name + \".txt\", \"w\")  \n",
    "    file.write(\"tipo de documento*data*urn*localidade*autoridade*\" + \\\n",
    "               \"tÃ­tulo*descricao*identifier*assuntos->\\n\")\n",
    "    for i in laws:\n",
    "        file.write(i.print_self())\n",
    "    file.close()\n",
    "        \n",
    "        \n",
    "\"\"\"    \n",
    "Scrapes all laws in url_base, in the range [1, 500 * n]. Returns a list \n",
    "of objects with each laws' attributes in laws[], and prints the result to \n",
    "file_name.txt. Entries are separated by *. If lexicon_flag is useg, a \n",
    "file_name_lexicon.txt is also generated with the lexicon of subjects in \n",
    "descending order of how often that subject as appeared.\n",
    "\"\"\"\n",
    "def scrape_site (url_base, laws, file_name, n, lexicon_flag = False):\n",
    "    for i in range(0, n):\n",
    "        url = url_base + str(i * 500 + 1)\n",
    "        req = requests.request('GET', url)\n",
    "        tree = etree.fromstring(req.content)\n",
    "\n",
    "        # x stands for each entry in <srw_dc:dc>\n",
    "        for x in tree.findall(\".//srw_dc:dc\", namespaces=ns):\n",
    "            new_law = get_values_test(x)\n",
    "            laws.append(new_law)\n",
    "            \n",
    "        # Being polite\n",
    "        time.sleep(2)\n",
    "            \n",
    "    if (lexicon_flag):\n",
    "        lexicon = get_lexicon(laws)\n",
    "        print_lexicon(lexicon, file_name)\n",
    "    \n",
    "    print_scraped_info(laws, file_name)\n",
    "    \n",
    "    return laws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url to scrape all federal laws (leis federais)\n",
    "#url = \"https://www.lexml.gov.br/busca/SRU?operation=searchRetrieve&query=urn+=%22lei+federal%22&maximumRecords=500&startRecord=\" \n",
    "\n",
    "# Url to scrape all law-decrees (decretos-lei)\n",
    "url = \"https://www.lexml.gov.br/busca/SRU?operation=searchRetrieve&query=urn+=%22federal+decreto.lei%22&maximumRecords=500&startRecord=\"\n",
    "\n",
    "laws = []\n",
    "laws = scrape_site(url, laws, \"leis2222\", 2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two lexicon files generated can be merged in the command line using:\n",
    "\n",
    "> sort file1.txt file2.txt | uniq > exit_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Receives a list of string with previously generated lexicon file names. \n",
    "Generates a file named merged_lexicon_files.txt in the current folder, \n",
    "with the sum of how many times each term has apparead in each lexicon file.\n",
    "\"\"\"\n",
    "def merge_lexicon_files (list_of_files):\n",
    "    lex = {}\n",
    "    for file in list_of_files:\n",
    "        f = open(file)\n",
    "        for line in f.readlines():\n",
    "            value, key = line.split(\"*\")\n",
    "            key = key.strip()\n",
    "            lex[key] = lex.get(key, 0) + int(value)\n",
    "            \n",
    "    lex = sorted(lex.items(), key = lambda l: l[1], reverse = True)\n",
    "    \n",
    "    file = open(\"merged_lexicon_files.txt\", \"w\")\n",
    "    for k, v in lex:\n",
    "        file.write(str(v) + \"*\" + k + \"\\n\")\n",
    "    file.close()\n",
    "            \n",
    "lex = merge_lexicon_files([\"decretos-lei-complete_lexicon.txt\", \"leis-complete_lexicon.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
